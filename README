# Titanic Dataset - Data Cleaning & Preprocessing

## Project Overview

This project is part of the **Elevate AI & ML Internship - Task 1: Data Cleaning & Preprocessing**. The objective is to learn and apply data cleaning, preprocessing, and feature engineering techniques on the Titanic dataset using Python.

---

## Dataset Information

**Dataset:** Titanic Dataset  
**Records:** 891 passengers  
**Original Features:** 12  
**Final Features:** 8 (after preprocessing)

### Original Columns:
- `PassengerId`: Unique passenger identifier
- `Survived`: Target variable (0 = Did not survive, 1 = Survived)
- `Pclass`: Ticket class (1 = First, 2 = Second, 3 = Third)
- `Name`: Passenger name
- `Sex`: Gender (male/female)
- `Age`: Age in years
- `SibSp`: Number of siblings/spouses aboard
- `Parch`: Number of parents/children aboard
- `Ticket`: Ticket number
- `Fare`: Ticket fare paid
- `Cabin`: Cabin number
- `Embarked`: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)

---

## Data Cleaning & Preprocessing Steps

### Step 1: Data Exploration
- Loaded dataset and examined basic information
- Identified data types and missing values
- Analyzed statistical distributions

**Missing Values Identified:**
- Age: 177 missing values (19.87%)
- Embarked: 2 missing values (0.22%)
- Cabin: 687 missing values (77.10%)

### Step 2: Handling Missing Values

| Column | Strategy | Result |
|--------|----------|--------|
| Age | Filled with median (28.0) | 0 missing |
| Embarked | Filled with mode ('S') | 0 missing |
| Cabin | Dropped entirely | Feature removed |
| PassengerId, Name, Ticket | Dropped (non-predictive) | Features removed |

**Rationale:**
- **Age & Embarked:** Small percentage of missing values, suitable for imputation
- **Cabin:** Too sparse (77%) to be useful for modeling
- **PassengerId, Name, Ticket:** Non-predictive identifiers

### Step 3: Categorical Encoding

Converted categorical features to numerical format using **Label Encoding**:

**Sex Column:**
- female → 0
- male → 1

**Embarked Column:**
- C (Cherbourg) → 0
- Q (Queenstown) → 1
- S (Southampton) → 2

**Why Label Encoding?** Suitable for ordinal relationships and machine learning algorithms that work with numerical data.

### Step 4: Outlier Detection

Using **Interquartile Range (IQR) Method** to identify outliers:

| Feature | Outliers Detected | Percentage |
|---------|-------------------|-----------|
| Age | 66 | 7.41% |
| Fare | 116 | 13.02% |
| SibSp | 46 | 5.16% |
| Parch | 213 | 23.91% |

**Decision:** Outliers were detected but not removed, as they represent legitimate real-world variations in passenger data.

### Step 5: Feature Scaling (Standardization)

Applied **StandardScaler** to numerical features:
- **Features scaled:** Age, Fare
- **Method:** Standardization (z-score normalization)
- **Formula:** \(z = \frac{x - \mu}{\sigma}\)

**Results:**
- Mean ≈ 0 (2.27e-16 for Age, 3.99e-18 for Fare)
- Standard Deviation ≈ 1 (1.0006 for both)

**Why standardization?** Ensures features have similar scales, improving model performance and convergence speed in ML algorithms.

---

## Final Preprocessed Dataset

### Characteristics:
- **Records:** 891 (unchanged)
- **Features:** 8 (reduced from 12)
- **Missing Values:** 0
- **Data Quality:** 100% complete

### Final Columns:
1. `Survived` - Target variable (int)
2. `Pclass` - Passenger class (int)
3. `Sex` - Gender encoded (int)
4. `Age` - Standardized (float)
5. `SibSp` - Family members count (int)
6. `Parch` - Family members count (int)
7. `Fare` - Standardized (float)
8. `Embarked` - Port encoded (int)

---

## Files Generated

### Outputs:
1. **`titanic_preprocessing.py`** - Complete Python script with all preprocessing steps
2. **`titanic_cleaned.csv`** - Dataset after missing value handling and encoding
3. **`titanic_preprocessed.csv`** - Final preprocessed dataset (with scaling)
4. **`outlier_detection_boxplots.png`** - Visualization of outliers
5. **`final_features.png`** - Visualization of final features
6. **`README.md`** - This documentation file

---

## Technologies & Libraries Used

```python
- pandas (v1.x+): Data manipulation and analysis
- numpy (v1.x+): Numerical computing
- matplotlib: Visualization
- seaborn: Statistical visualization
- scikit-learn: Machine learning tools (preprocessing)
```

---

## Key Concepts Applied

### 1. Missing Data Types
- **Missing Completely at Random (MCAR):** No pattern in missingness
- **Missing at Random (MAR):** Missingness related to observed data
- **Missing Not at Random (MNAR):** Missingness related to unobserved data

### 2. Handling Strategies
- **Mean/Median Imputation:** For numerical features with small missing percentage
- **Mode Imputation:** For categorical features
- **Deletion:** When missing data exceeds 50-70% threshold

### 3. Categorical Encoding
- **Label Encoding:** Assigns integer values to categories
- **One-Hot Encoding:** Creates binary columns for each category
- **Target Encoding:** Based on target variable mean

### 4. Feature Scaling
- **Normalization (0-1 range):** \(x_{norm} = \frac{x - x_{min}}{x_{max} - x_{min}}\)
- **Standardization (z-score):** \(x_{std} = \frac{x - \mu}{\sigma}\)

### 5. Outlier Detection
- **IQR Method:** Q1 - 1.5×IQR to Q3 + 1.5×IQR
- **Z-score Method:** |z| > 3 indicates outlier
- **Visual Inspection:** Boxplots and scatter plots

---

## Interview Question Answers

### Q1: What are the different types of missing data?
**Answer:** Three types:
- **MCAR:** Missing completely at random, no pattern
- **MAR:** Missing at random, pattern related to other variables
- **MNAR:** Not random, related to unobserved data (bias risk)

### Q2: How do you handle categorical variables?
**Answer:** Common techniques:
- Label Encoding: Converts categories to integers (0, 1, 2...)
- One-Hot Encoding: Creates binary columns for each category
- Target Encoding: Uses target variable mean for each category
- Ordinal Encoding: For ordered categories

### Q3: What is the difference between normalization and standardization?
**Answer:**
- **Normalization:** Scales data to 0-1 range, uses min-max
- **Standardization:** Centers data around 0 with unit variance, uses z-score
- Use normalization for bounded features; standardization for normally distributed data

### Q4: How do you detect outliers?
**Answer:** Methods include:
- **IQR Method:** Values beyond Q1-1.5×IQR or Q3+1.5×IQR
- **Z-score:** |z| > 3 (99.7% within range)
- **Visual Methods:** Boxplots, scatter plots
- **Statistical Tests:** Grubbs' test, Isolation Forest

### Q5: Why is preprocessing important in ML?
**Answer:**
- Improves model accuracy and stability
- Reduces training time and computational cost
- Handles data inconsistencies and errors
- Ensures fair feature weighting
- Reduces overfitting risk

### Q6: What is one-hot encoding vs label encoding?
**Answer:**
- **Label Encoding:** Converts categories to integers (0, 1, 2), assumes ordinal relationship
- **One-Hot Encoding:** Creates binary columns for each category, suitable for nominal data
- Use label encoding for tree-based models; one-hot for linear models

### Q7: How do you handle data imbalance?
**Answer:**
- **Under-sampling:** Reduce majority class
- **Over-sampling:** Increase minority class using SMOTE
- **Class Weights:** Penalize minority class errors more
- **Stratified Sampling:** Maintain class distribution in splits

### Q8: Can preprocessing affect model accuracy?
**Answer:** Yes, significantly:
- Proper handling of missing values improves predictions
- Feature scaling essential for distance-based algorithms
- Categorical encoding affects model interpretability
- Outlier removal can reduce noise or lose important patterns
- Feature selection/removal improves generalization

---

## Usage Instructions

### Running the Script:
```bash
python titanic_preprocessing.py
```

### Expected Output:
- Console output showing step-by-step preprocessing progress
- Generated CSV files with cleaned and preprocessed data
- Visualization images showing outliers and features

### Loading Preprocessed Data:
```python
import pandas as pd

# Load final preprocessed data
df_final = pd.read_csv('titanic_preprocessed.csv')

# Ready for ML model training
# Use as input to classification algorithms
```

---

## Performance Metrics

### Data Quality Improvement:
- **Missing Data Reduction:** 866 → 0 missing values (100% improvement)
- **Features Reduction:** 12 → 8 features (33% reduction)
- **Data Completeness:** 88.1% → 100%

### Preprocessing Impact:
- Removed non-predictive columns reducing model complexity
- Imputed missing values preserving 891 records
- Scaled features ensuring equal contribution
- Encoded categorical variables for ML compatibility

---

## Conclusion

This preprocessing pipeline demonstrates professional-grade data cleaning and preparation techniques suitable for machine learning projects. The Titanic dataset has been transformed from raw, messy data to a clean, analysis-ready format with:

✓ 100% missing value handling
✓ Proper categorical encoding
✓ Standardized numerical features
✓ Outlier detection and analysis
✓ Complete documentation

The preprocessed dataset is now ready for exploratory data analysis, feature engineering, model training, and evaluation.

---

## References

- Pandas Documentation: https://pandas.pydata.org/
- Scikit-learn Preprocessing: https://scikit-learn.org/stable/modules/preprocessing.html
- Titanic Dataset: https://www.kaggle.com/c/titanic
- Data Cleaning Best Practices: https://towardsdatascience.com/

---

## Author Notes

This project demonstrates core competencies in:
- Data analysis and manipulation
- Statistical knowledge
- Feature engineering
- Python programming
- Data visualization
- Documentation skills

Suitable for ML engineers, data scientists, and analytics professionals.

---

**Last Updated:** November 13, 2025  
**Status:** Complete ✓
